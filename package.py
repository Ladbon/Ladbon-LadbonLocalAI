import os
import shutil
import sys
import subprocess
import PyInstaller.__main__
import platform

# Check and install required packages if needed
try:
    import PyInstaller
except ImportError:
    print("Installing PyInstaller...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "pyinstaller"])

try:
    import psutil
except ImportError:
    print("Installing psutil (required for llamacpp_client)...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "psutil"])

# Define paths
SRC_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SRC_DIR)
APP_NAME = "Ladbon AI Desktop"

# Move to the correct directory
os.chdir(SRC_DIR)

# Clean previous builds
if os.path.exists(os.path.join(PROJECT_ROOT, "build")):
    shutil.rmtree(os.path.join(PROJECT_ROOT, "build"))
if os.path.exists(os.path.join(PROJECT_ROOT, "dist")):
    shutil.rmtree(os.path.join(PROJECT_ROOT, "dist"))

# Create necessary directories that should be included
for dir_name in ["docs", "img", "logs", "models"]:
    dir_path = os.path.join(PROJECT_ROOT, dir_name)
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
        print(f"Created directory: {dir_path}")

# Create a dummy settings file if it doesn't exist
settings_path = os.path.join(PROJECT_ROOT, "settings.json")
if not os.path.exists(settings_path):
    with open(settings_path, "w") as f:
        f.write('{"model": "qwen3:8b", "max_tokens": 8192, "custom_system_prompt": "", "timeout": 0}')
    print(f"Created default settings file")

# Set icon path if ladbon_ai.ico exists in project root
icon_path = os.path.join(PROJECT_ROOT, "ladbon_ai.ico")
if os.path.exists(icon_path):
    print(f"Using custom icon: {icon_path}")
else:
    print("Custom icon 'ladbon_ai.ico' not found, using default icon.")
    icon_path = None

# Add icon to PyInstaller arguments if available
# (Moved below after pyinstaller_args is defined)

def prepare_llama_cpp():
    """Prepares the llama-cpp-python package for PyInstaller bundling"""
    try:
        import llama_cpp
        has_llama_cpp = True
        print("llama-cpp-python is installed")
        
        # Get the llama_cpp package path to include its libraries
        llamacpp_path = os.path.dirname(llama_cpp.__file__)
        print(f"llama_cpp package path: {llamacpp_path}")
        
        # Check for lib directory
        llamacpp_lib_path = os.path.join(llamacpp_path, "lib")
        
        # Check if lib directory exists
        if os.path.exists(llamacpp_lib_path):
            print(f"Found llama-cpp lib directory at: {llamacpp_lib_path}")
            
            # List all files in lib directory and subdirectories
            import glob
            lib_files = []
            for path in glob.glob(os.path.join(llamacpp_lib_path, "**"), recursive=True):
                if os.path.isfile(path):
                    lib_files.append(os.path.relpath(path, llamacpp_lib_path))
            
            if lib_files:
                print(f"Found {len(lib_files)} files in lib directory")
                for file in lib_files[:5]:  # Print only first 5 files to avoid overwhelming output
                    print(f" - {file}")
                if len(lib_files) > 5:
                    print(f" ... and {len(lib_files) - 5} more files")
            else:
                print("WARNING: lib directory exists but is empty")
        else:
            print(f"WARNING: llama-cpp lib directory not found at {llamacpp_lib_path}")
            # Create a lib directory if it doesn't exist
            os.makedirs(llamacpp_lib_path, exist_ok=True)
            print(f"Created empty lib directory at {llamacpp_lib_path}")
            
        # Create a helper file to locate the lib directory at runtime
        with open(os.path.join(llamacpp_path, "lib_path.py"), "w") as f:
            f.write('''
# This file is auto-generated by package.py
import os
import sys
import logging

logger = logging.getLogger('llama_cpp.lib_path')

def get_lib_path():
    """Returns the path to the llama_cpp library directory"""
    if getattr(sys, '_MEIPASS', None):
        # Running as a PyInstaller bundle
        bundle_lib_path = os.path.join(sys._MEIPASS, 'llama_cpp', 'lib')
        logger.info("Running as PyInstaller bundle, lib path: %s", bundle_lib_path)
        return bundle_lib_path
    else:
        # Running as a normal Python script
        normal_lib_path = os.path.join(os.path.dirname(__file__), 'lib')
        logger.info("Running as normal Python script, lib path: %s", normal_lib_path)
        return normal_lib_path
''')
        print("Created lib_path.py helper file")
        
        return has_llama_cpp, llamacpp_path, llamacpp_lib_path
    except ImportError:
        print("WARNING: llama-cpp-python is not installed.")
        print("Local model inference will not be available.")
        print("Run install_llamacpp.py first if you want local model support.")
        return False, None, None

# Check if llama-cpp-python is installed and prepare it
has_llama_cpp, llamacpp_path, llamacpp_lib_path = prepare_llama_cpp()

# Command line arguments
import argparse
parser = argparse.ArgumentParser(description='Build Ladbon AI Desktop application')
parser.add_argument('--onefile', action='store_true', help='Build as a single executable file')
args = parser.parse_args()

# Build the executable
pyinstaller_args = [
    'gui_app.py',  # Your main entry point file
    f'--name={APP_NAME}',
    '--onefile' if args.onefile else '--onedir',  # Use onefile if specified, otherwise onedir
    '--windowed',
    '--add-data={}{}docs;docs'.format(PROJECT_ROOT, os.sep),
    '--add-data={}{}img;img'.format(PROJECT_ROOT, os.sep),
    '--add-data={}{}logs;logs'.format(PROJECT_ROOT, os.sep),
    '--add-data={}{}models;models'.format(PROJECT_ROOT, os.sep),  # Include models directory
    '--add-data={}{}settings.json;.'.format(PROJECT_ROOT, os.sep),
    '--collect-binaries=llama_cpp',  # Collect all binary files from llama_cpp package
    # '--copy-metadata=llama_cpp',   # Removed - llama_cpp doesn't have standard metadata
    '--hidden-import=PyQt5.QtPrintSupport',  # Required for QTextEdit
    '--hidden-import=utils.ollama_client',
    '--hidden-import=utils.llamacpp_client',  # New llamacpp client
    '--hidden-import=utils.llamacpp_loader',  # Important! Loader utility for llama-cpp
    '--hidden-import=utils.huggingface_manager',  # New HuggingFace manager
    '--hidden-import=utils.model_manager',  # New model manager
    '--hidden-import=utils.sanitycheck',  # New sanity check utility
    '--hidden-import=cli.doc_handler',
    '--hidden-import=cli.img_handler',
    '--hidden-import=cli.web_search',
    '--hidden-import=cli.rag',  # Include RAG module
    '--hidden-import=utils.logger',
    '--hidden-import=utils.numpy_init_fix',  # Add our NumPy fix module
    '--hidden-import=importlib',
    '--hidden-import=ctypes',
    '--hidden-import=logging',
    '--hidden-import=numpy',  # Explicitly include NumPy to ensure it's bundled correctly
]

# Add icon to PyInstaller arguments if available
if icon_path:
    pyinstaller_args.append(f'--icon={icon_path}')

# Add manifest file for better Windows DLL handling
manifest_path = os.path.join(SRC_DIR, "ladbon_ai_desktop.manifest")
if os.path.exists(manifest_path):
    print(f"Using manifest file: {manifest_path}")
    pyinstaller_args.append(f'--manifest={manifest_path}')
# Add llama-cpp-python related imports if available
if has_llama_cpp:
    pyinstaller_args.extend([
        '--hidden-import=llama_cpp',
        '--hidden-import=llama_cpp.llama_cpp',
        '--hidden-import=llama_cpp._ctypes_extensions',
        '--hidden-import=llama_cpp.lib_path',  # Our auto-generated helper
        '--hidden-import=utils.llamacpp_loader',  # Our loader utility
        '--hidden-import=psutil',  # Required by llamacpp_client
        '--hidden-import=huggingface_hub',  # Required for model download
        '--hidden-import=tqdm',  # Required for progress bars
    ])
    
    # Add llama_cpp directory and all its contents
    if llamacpp_path:
        # Add entire llama_cpp package as data
        print(f"Adding llama_cpp package from {llamacpp_path}")
        pyinstaller_args.append(f'--add-data={llamacpp_path}{os.sep}*.py;llama_cpp')
        
        # Add llama_cpp/lib directory and all its contents if it exists
        if llamacpp_lib_path and os.path.exists(llamacpp_lib_path):
            print(f"Adding llama_cpp lib directory from {llamacpp_lib_path}")
            
            # Add the lib directory and its contents as binary files
            # This is critical to ensure the library loads correctly
            pyinstaller_args.append(f'--add-binary={llamacpp_lib_path}{os.sep}*;llama_cpp{os.sep}lib')
            
            # For Windows, ensure we copy DLLs with exact destination paths
            if platform.system() == "Windows":
                # Create a recursive list of all files in lib directory
                for root, dirs, files in os.walk(llamacpp_lib_path):
                    for file in files:
                        if file.endswith('.dll'):
                            file_path = os.path.join(root, file)
                            # Copy to root directory as well for fallback
                            print(f"Adding DLL to root directory: {file}")
                            pyinstaller_args.append(f'--add-binary={file_path};.')
            
            # Also add the empty directory structure (important for PyInstaller to create the directory)
            pyinstaller_args.append(f'--add-data={llamacpp_lib_path}{os.sep};llama_cpp{os.sep}lib')
        
    # Add all DLLs from site-packages directory that might be needed
    import site
    site_packages = site.getsitepackages()[0]
    
    # For Windows, specifically look for DLLs that might be needed
    if platform.system() == "Windows":
        print("Adding Windows-specific DLLs")
        import glob
        
        # Add all DLLs from site-packages directory 
        dll_files = glob.glob(os.path.join(site_packages, "*.dll"))
        for dll_file in dll_files:
            print(f"Adding DLL from site-packages: {os.path.basename(dll_file)}")
            pyinstaller_args.append(f'--add-binary={dll_file};.')

# Check for platform-specific requirements
if platform.system() == "Windows":
    pyinstaller_args.append('--hidden-import=win32process')

# Add additional hooks path for custom hooks
pyinstaller_args.append(f'--additional-hooks-dir={SRC_DIR}')

# Add our NumPy fix as a runtime hook to be executed before any imports
pyinstaller_args.append(f'--runtime-hook={os.path.join(SRC_DIR, "pre_numpy_fix.py")}')

# Always add the clean flag at the end
pyinstaller_args.append('--clean')

# Run PyInstaller with our arguments
print("Building the executable... (this may take a few minutes)")
PyInstaller.__main__.run(pyinstaller_args)

print(f"\nBuild completed! The executable is in the 'dist' folder.")

# Create a launcher batch file for the onedir build
dist_dir = os.path.join(PROJECT_ROOT, "dist", APP_NAME)

# Make sure the dist directory exists
os.makedirs(os.path.join(PROJECT_ROOT, "dist"), exist_ok=True)

launcher_path = os.path.join(PROJECT_ROOT, "dist", f"Launch {APP_NAME}.bat")

with open(launcher_path, "w") as f:
    f.write(f'@echo off\n')
    f.write(f'echo Starting {APP_NAME}...\n')
    f.write(f'cd "%~dp0{APP_NAME}"\n')
    f.write(f'start "" "{APP_NAME}.exe"\n')

print(f"Created launcher batch file: {launcher_path}")

# Print appropriate messages about model support
if has_llama_cpp:
    print(f"Local LLM support is included via llama-cpp-python.")
    print(f"Copy your GGUF models to the 'models' folder in the dist/{APP_NAME} directory")
else:
    print(f"WARNING: Local LLM support via llama-cpp-python was NOT included in this build.")
    print(f"Run install_llamacpp.py and rebuild if you want local model support.")

print(f"NOTE: You can also install Ollama separately for additional model options: https://ollama.com/download")
print(f"Remember to copy any document or image files you need to the 'docs' and 'img' folders in the dist/{APP_NAME} directory")
print(f"\nTo run the application, use the 'Launch {APP_NAME}.bat' file in the dist directory")